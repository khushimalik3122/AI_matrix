# -*- coding: utf-8 -*-
"""AI_Matrix (7).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZfiFrwG-bEwrI49poF6mnDyJc8s1vXN
"""

from google.colab import drive
drive.mount('/content/drive')

pip install transformers

import os
from huggingface_hub import login
from google.colab import userdata

# Accessing the secret key from Colab secrets
hf_token = userdata.get("HF_API_KEY")

if hf_token:
    login(token=hf_token)
else:
    raise ValueError("Hugging Face token not found in Colab secrets 'Colab_HF_token'")

import torch
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import os

"""**Load and Preprocess Training Data**"""

test_df = pd.read_csv('/content/drive/MyDrive/test (6).csv')
test_df.head()

train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Sentinel2_CropSamples_Reorganized.csv')
train_df.head()

train_df['crop_type'].value_counts()

from transformers import AutoModel, AutoConfig

model_name = "AminiTech/amini-28M-v1"

# Load model config and model (for PatchTST, not a language model!)
config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, config=config, trust_remote_code=True)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import numpy as np

def mean_aggregator(cls_embeddings):
    """
    Fully averages all CLS embeddings, no splitting.
    Handles extra dimensions safely.

    Args:
        cls_embeddings: list or array of CLS tokens [n, 512] or [1, n, 512]
    Returns:
        final_embedding: mean [512]-dim vector
    """
    cls_embeddings = np.array(cls_embeddings)

    # Handle possible extra dimensions
    if cls_embeddings.ndim == 3 and cls_embeddings.shape[1] == 1:
        cls_embeddings = cls_embeddings.squeeze(1)
    elif cls_embeddings.ndim == 1:
        cls_embeddings = cls_embeddings.reshape(1, -1)  # shape: [1, 512]

    return np.mean(cls_embeddings, axis=0)

def generate_embeddings_sliding_window(df, model, max_length=48, stride=24):
    grouped = df.groupby('unique_id')
    all_embeddings = {}

    for unique_id, group in grouped:
        group = group.sort_values('time')
        features = group[['blue', 'green', 'nir', 'nir08', 'red', 'rededge1',
                          'rededge2', 'rededge3', 'swir16', 'swir22']].values
        sequence_length = len(features)
        print(f"[{unique_id}] Raw sequence length: {sequence_length}")

        # Normalize
        features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-6)
        cls_list = []

        if sequence_length < max_length:
            # Pad to 48
            pad_len = max_length - sequence_length
            padding = np.zeros((pad_len, features.shape[1]))
            features_padded = np.vstack([features, padding])

            tensor_input = torch.tensor(features_padded, dtype=torch.float32).unsqueeze(0).to(device)
            print(f"[{unique_id}] Padded input shape: {tensor_input.shape}")

            with torch.no_grad():
                try:
                    output = model(tensor_input)
                    print(f"[{unique_id}] Output type: {type(output)}")
                    print(f"[{unique_id}] Output shape: {output.last_hidden_state.shape}")

                    # ✅ CLS patch (index 0), mean across bands
                    cls_token = output.last_hidden_state[:, :, 0, :].mean(dim=1).squeeze(0).cpu().numpy()
                    assert cls_token.shape == (512,)
                    cls_list.append(cls_token)

                except Exception as e:
                    print(f"[{unique_id}] Error during padded input: {type(e).__name__} - {e}")
                    continue

        else:
            # Sliding windows
            for i in range(0, sequence_length - max_length + 1, stride):
                window = features[i : i + max_length]
                tensor_input = torch.tensor(window, dtype=torch.float32).unsqueeze(0).to(device)
                print(f"[{unique_id}] Window {i} input shape: {tensor_input.shape}")

                with torch.no_grad():
                    try:
                        output = model(tensor_input)
                        print(f"[{unique_id}] Output type: {type(output)}")
                        print(f"[{unique_id}] Output shape: {output.last_hidden_state.shape}")

                        # ✅ CLS patch token = patch index 0
                        cls_token = output.last_hidden_state[:, :, 0, :].mean(dim=1).squeeze(0).cpu().numpy()
                        assert cls_token.shape == (512,)
                        cls_list.append(cls_token)

                    except Exception as e:
                        print(f"[{unique_id}] Error at window {i}: {type(e).__name__} - {e}")
                        continue

        if cls_list:
            cls_array = np.array(cls_list)
            print(f"[{unique_id}] CLS token array shape: {cls_array.shape}")  # e.g., (3, 512)
            all_embeddings[unique_id] = cls_list
        else:
            print(f"[{unique_id}] No CLS tokens generated")

    return all_embeddings

cls_embeddings = generate_embeddings_sliding_window(train_df, model, max_length=48, stride=24)

cls_embeddings_test = generate_embeddings_sliding_window(test_df, model, max_length=48, stride=24)

for uid, cls_list in cls_embeddings.items():
    arr = np.array(cls_list)
    print(f"{uid} → shape: {arr.shape}")

final_train_embeddings = []
labels = []

for uid, cls_list in cls_embeddings.items():
    agg_embedding = mean_aggregator(cls_list)  # ✅ Full mean pooling, no splitting
    final_train_embeddings.append(agg_embedding)

    crop_type = train_df.loc[train_df['unique_id'] == uid, 'crop_type'].iloc[0]
    labels.append(crop_type)

final_test_embeddings = []
test_uids = []

for uid, cls_list in cls_embeddings_test.items():
    agg_embedding = mean_aggregator(cls_list)  # ✅ Full mean over all CLS tokens
    final_test_embeddings.append(agg_embedding)
    test_uids.append(uid)  # Optional: useful for mapping back later

import numpy as np

# Updated label map with correct crop type names
label_map = {'cocoa': 0, 'rubber': 1, 'oil': 2}  # 'oil' instead of 'oil_palm'

# Extract the first crop_type per unique_id
pixel_labels = train_df.groupby('unique_id').first()['crop_type']

# Map crop types to integer labels
labels = pixel_labels.map(label_map)

# Drop any unmapped values (NaN) and convert to integer array
labels = labels.dropna().astype(int).values

# Output shape and distribution
print("Labels shape:", labels.shape)
print("Label distribution:", np.bincount(labels))

import numpy as np

X_train = np.array(final_train_embeddings)  # shape: [num_pixels, 512]
y_train = np.array(labels)  # shape: [num_pixels]

print(X_train.shape, y_train.shape)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)

# Optional: get label → crop name mapping
class_names = le.classes_

X_train = np.array(final_train_embeddings)

# Flatten from shape [N, 1, 512] → [N, 512] if needed
if X_train.ndim == 3 and X_train.shape[1] == 1:
    X_train = X_train.squeeze(1)

print("X_train shape:", X_train.shape)  # should be (N, 512)
print("y_train shape:", y_train.shape)  # should be (N,)

import numpy as np
from sklearn.linear_model import LogisticRegression

X_train = np.array(final_train_embeddings)
y_train = np.array(y_train_encoded)

# ✅ FIX THE SHAPE
if X_train.ndim == 3 and X_train.shape[1] == 1:
    X_train = X_train.squeeze(1)

from sklearn.model_selection import train_test_split

X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.3, random_state=42, stratify=y_train
)


clf = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial',class_weight='balanced')
clf.fit(X_tr, y_tr)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_pred_val = clf.predict(X_val)

print(f"Validation Accuracy: {accuracy_score(y_val, y_pred_val):.4f}")
print(classification_report(y_val, y_pred_val))

cm = confusion_matrix(y_val, y_pred_val)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Validation Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X_tr, y_tr)

from sklearn.metrics import log_loss

# Predict class probabilities on validation set
y_proba_val = clf.predict_proba(X_val)

# Compute log loss
val_log_loss = log_loss(y_val, y_proba_val)

print(f"Validation Log Loss: {val_log_loss:.4f}")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predict using trained model
y_pred = clf.predict(X_train)

# Accuracy
acc = accuracy_score(y_train, y_pred)
print(f"Accuracy: {acc:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_train, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_train, y_pred)

# Plot it
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.tight_layout()
plt.show()

# 1️⃣ Ensure X_test is ready
X_test = np.array(final_test_embeddings)
if X_test.ndim == 3 and X_test.shape[1] == 1:
    X_test = X_test.squeeze(1)

# 2️⃣ Get class probabilities using predict_proba
probs = clf.predict_proba(X_test)  # shape: [N, 3]

# 3️⃣ Map columns to class names from LabelEncoder
class_names = le.inverse_transform(np.arange(probs.shape[1]))  # ['cocoa', 'rubber', 'oil']

# 4️⃣ Create DataFrame with unique_id and class probabilities
probs_df = pd.DataFrame(probs, columns=class_names)
probs_df.insert(0, "unique_id", test_uids)  # Add UIDs to front

# 5️⃣ Optional: round for presentation
probs_df = probs_df.round(3)

# Preview
print(probs_df.head())

probs_df.to_csv("test_probabilities.csv", index=False)



